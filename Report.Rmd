---
title: "Imbalanced Classification - Loan request dataset"
author: "Ahmet Zamanis"
date: "2022-10-12"
output: 
  github_document:
    toc: yes
---

```{r setup, include=FALSE}

knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(message = FALSE)
knitr::opts_chunk$set(warning = FALSE)

options(scipen=999)

```

## Introduction

## Data preparation

```{r Packages}

library(tidyverse) #data handling & ggplot2
library(GGally) #exploratory analysis
library(DataExplorer) #exploratory analysis
library(patchwork) #combining ggplot2 objects
library(gt) #data tables
library(tidymodels) #preprocessing
library(embed) #encoding
library(mlr3verse) #loads various mlr3 machine learning packages
library(mlr3hyperband) #hyperband tuning algorithm
library(xgboost) #regularized gradient boosting

```


```{r LoadData}

#load and summarize original data
df <- read.csv("./OriginalData/clients.csv", header=TRUE)
summary(data.frame(unclass(df), stringsAsFactors = TRUE))

```


```{r RenameColumns}

#rename columns
names(df)[8] <- "have_children"
names(df)[13] <- "prev_client"
names(df)[14] <- "bad_client"

```


```{r RecodeEducation}

#recode levels of education column
df$education <- recode(df$education, "Higher education"="Higher",
                       "Incomplete higher education"="HigherDropout",
                       "Incomplete secondary education"="Secondary",
                       "PhD degree"="Higher",
                       "Secondary education"="Secondary",
                       "Secondary special education"="SecondarySpec")

```


```{r RecodeProductType}

#recode levels of product_type column, combining unfrequent levels together
#new levels: Beauty, Vehicles, Necessities
df$product_type <- recode(df$product_type, 
                          "Cosmetics and beauty services" = "Beauty",
                          "Jewelry" = "Beauty",
                          "Auto" = "Vehicles",
                          "Boats" = "Vehicles",
                          "Medical services" = "Necessities",
                          "Training" = "Necessities",
                          "Childen's goods" = "Necessities"
                          )


#new level: Recreational
lookup_recreational <- tibble(old=c("Tourism", "Music", "Fishing and hunting supplies",
                                    "Sporting goods", "Audio & Video", "Fitness"),
                              new=c(rep("Recreational", 6)))
df$product_type <- recode(df$product_type, !!!deframe(lookup_recreational))


#new level: Hardware
lookup_hardware <- tibble(old=c("Windows & Doors", "Construction Materials", "Repair Services",
                                "Garden equipment"),
                              new=c(rep("Hardware", 4)))
df$product_type <- recode(df$product_type, !!!deframe(lookup_hardware))


#shorten the level Household appliances
df$product_type <- recode(df$product_type, "Household appliances"="Appliances")

#remove intermediary objects
rm(prod_types, prod_types_rare, lookup_hardware, lookup_recreational)

```


```{r FactorConversions}

#factor convert categorical columns
#month, sex, product_type, family status, region, phone operator: 
  #factorize, don't change levels
for (i in c(1, 5, 7, 9, 11, 12)){
  df[,i] <- as.factor(df[,i])
}
rm(i)


#have children, region, phone_operator, prev_client, bad_client
  #factorize, recode levels
for (i in c(8, 13, 14)) {
  df[,i] <- recode(as.character(df[,i]),
                   "0" = "No",
                   "1" = "Yes")
  df[,i] <- as.factor(df[,i])
}
rm(i)


#education: factorize and order levels
df$education <- factor(df$education, levels=c("SecondarySpec", "Secondary",
                                              "HigherDropout", "Higher"),
                       ordered=TRUE)

```


```{r SummaryDataPrep}

summary(df)

```


## Exploratory analysis

### Numeric features

```{r EDANumerics, echo=FALSE}

ggpairs(df,
        columns=c("credit_amount", "credit_term", "age", "income", "bad_client"),
        mapping=(aes(color=bad_client, fill=bad_client,  alpha=0.2)),
        title="Numeric features and outcome") +
  scale_color_manual(values=c("#00BFC4", "#F8766D")) +
  scale_fill_manual(values=c("#00BFC4", "#F8766D"))

```


### Categorical features

```{r EDACatLoop, echo=FALSE}

plotno <- 1
for (i in c(1, 5, 6:9, 11:13)){
  y_val <- df[,i]
  p <- ggplot(df, aes(y=!!y_val, color=bad_client, fill=bad_client)) +
    geom_bar(position="stack", alpha=0.5) +
    scale_color_manual(values=c("#00BFC4", "#F8766D")) +
    scale_fill_manual(values=c("#00BFC4", "#F8766D"))
  
  assign(paste0("bar", plotno), p)
  plotno <- plotno + 1
}
rm(p, plotno, i, y_val)

bar1 <- bar1 + labs(y="month", x="")
bar2 <- bar2 + labs(y="sex", x="")
bar3 <- bar3 + labs(y="education", x="Count")
bar4 <- bar4 + labs(y="product_type", x="Count")
bar5 <- bar5 + labs(y="have_children", x="")
bar6 <- bar6 + labs(y="region", x="")
bar9 <- bar9 + labs(y="prev_client", x="")
bar8 <- bar8 + labs(y="family_status", x="Count")
bar9 <- bar9 + labs(y="phone_operator", x="Count")

```

```{r EDACat1, echo=FALSE}

(bar1 | bar2) / (bar3 | bar4) +
  plot_layout(guides="collect") +
  plot_annotation(title="Categorical features, colored by outcome")

```


```{r EDACat2, echo=FALSE}

(bar5 | bar6 | bar9) / (bar7 | bar8) +
  plot_layout(guides="collect") +
  plot_annotation(title="Categorical features, colored by outcome")

```


### Interaction terms

```{r AgeXIncome, echo=FALSE}

ggplot(df, aes(age, income, color=bad_client)) + geom_point() +
  geom_smooth() + 
  labs(title="Interaction of age and income for loan request classification") + 
  scale_color_manual(values=c("#00BFC4", "#F8766D")) +
  scale_fill_manual(values=c("#00BFC4", "#F8766D")) +
  theme_bw()

```


```{r AgeXFamilyStatus, echo=FALSE}

ggplot(df, aes(family_status, age, fill=bad_client)) + 
  geom_boxplot(width=0.5, lwd=0.5) + stat_boxplot(geom="errorbar", width=0.5, lwd=0.5) +
  labs(title="Interaction of age and family status for loan request classification") + 
  scale_color_manual(values=c("#00BFC4", "#F8766D")) +
  scale_fill_manual(values=c("#00BFC4", "#F8766D")) +
  theme_bw()

```


```{r AgeXHaveChildren, echo=FALSE}

ggplot(df, aes(have_children, age, fill=bad_client)) + 
  geom_boxplot(width=0.5, lwd=0.5) + stat_boxplot(geom="errorbar", width=0.5, lwd=0.5) +
  labs(title="Interaction of age and having children for loan request classification") + 
  scale_color_manual(values=c("#00BFC4", "#F8766D")) +
  scale_fill_manual(values=c("#00BFC4", "#F8766D")) +
  theme_bw()

```


```{r CreditAmountXProductType, echo=FALSE}

ggplot(df, aes(product_type, credit_amount, fill=bad_client)) + 
  geom_boxplot(width=0.5, lwd=0.5) + stat_boxplot(geom="errorbar", width=0.5, lwd=0.5) +
  labs(title="Interaction of credit amount and product type for loan request classification") + 
  scale_color_manual(values=c("#00BFC4", "#F8766D")) +
  scale_fill_manual(values=c("#00BFC4", "#F8766D")) +
  theme_bw()

```


## Feature engineering

### Creating new features


```{r FeatEng}

#payment: amount to be paid in 1 payment period
df <- df %>% mutate(payment = round(credit_amount / credit_term, 0), .after=credit_term)

#ratio_amount: total amount to be paid / income
df <- df %>% mutate(ratio_amount = round(credit_amount / income, 2), .after=credit_amount)

#ratio_payment: one payment / income
df <- df %>% mutate(ratio_payment = round(payment / income, 2), .after=payment)

```


### Evaluating new features

```{r FeatEngEval, echo=FALSE}

ggpairs(df,
        columns=c("payment", "ratio_amount", "ratio_payment", "bad_client"),
        mapping=(aes(color=bad_client, fill=bad_client, alpha=0.5)),
        title="New features and outcome") +
        scale_color_manual(values=c("#00BFC4", "#F8766D")) +
        scale_fill_manual(values=c("#00BFC4", "#F8766D"))

```


## Modeling

### Preprocessing

```{r OrdinalEncode}

encode_ordinal <- function(x, order = unique(x)) {
  x <- as.numeric(factor(x, levels = order, exclude = NULL)) - 1
  x
}

df$education <- encode_ordinal(df$education, order=levels(df$education))

```


```{r PipelinePreproc}

#create preprocessing pipeline

#yeo-johnson transformation for numeric features
pipe_yeo <- po("yeojohnson", standardize=FALSE)


#weight of evidence encoding for nominal features
encode_woe <- po("encodeimpact")


#centering and scaling for numeric features
pipe_center_scale <- po("scale")


#add interaction terms with model matrix
pipe_interact <- po("modelmatrix", formula = ~ . + credit_amount:product_type.No +
                      credit_amount:product_type.Yes +
                      age:income + age:family_status.No + age:family_status.Yes +
                      age:have_children.No + age:have_children.Yes)


#add minority class weights
pipe_weights <- po("classweights")
pipe_weights$param_set$values$minor_weight <- summary(df$bad_client)[1] /
  summary(df$bad_client)[2]


#create combined preprocessing pipeline (Graph in mlr3)
graph_preproc <- pipe_yeo %>>% encode_woe %>>% pipe_center_scale %>>%
  pipe_interact %>>% pipe_weights

```


```{r mlr3Objects}

#create classification task
task_credit <- as_task_classif(df, target="bad_client", positive="Yes")


#create 5-folds resampling
resample_cv <- rsmp("cv", folds=5L)


#create performance measures
source("mlr3CustomMeasures.R") #source custom implementation of Cohen's kappa measure
measure_rec <- msr("classif.recall") 
measure_prauc <- msr("classif.prauc") 
measure_brier <- msr("classif.bbrier") 
measure_kappa <- msr("classif.kappa") 
measure_acc <- msr("classif.acc") 
measure_time <- msr("time_both")
measures_list <- c(measure_rec, measure_prauc, measure_brier, measure_kappa, 
                   measure_acc, measure_time)


#logloss, just as a tuning metric
measure_log <- msr("classif.logloss")


#create tuners
tune_grid <- mlr3tuning::tnr("grid_search")
tune_random <- mlr3tuning::tnr("random_search") 
tune_hyper2 <- mlr3tuning::tnr("hyperband", eta=2) 
tune_hyper3 <- mlr3tuning::tnr("hyperband", eta=3) 
tune_anneal <- mlr3tuning::tnr("gensa") 


#create tuning terminators
term10 <- trm("evals", n_evals=10)
term25 <- trm("evals", n_evals=25)
term50 <- trm("evals", n_evals=50)
term100 <- trm("evals", n_evals=100)
term250 <- trm("evals", n_evals=250)
term500 <- trm("evals", n_evals=500)
term_none <- trm("none")

```


### Approach to hyperparameter tuning

```{r TuneAnneal, eval=FALSE}

#create preprocessing + model pipeline (GraphLearner in mlr3)
learner_glmnet <- as_learner(graph_preproc %>>% po("learner", 
                                                   learner=lrn("classif.glmnet",
                                                   predict_type="prob")))


#create a search space for tuning
space_glmnet <- ps(
  classif.glmnet.alpha=p_dbl(lower=0, upper=1),
  classif.glmnet.lambda=p_dbl(lower=0, upper=1)
)


#create autotuner object
autotune_glmnet = AutoTuner$new(
  learner=learner_glmnet,
  resampling=resample_cv,
  measure=measure_log,
  terminator=term100,
  tuner=tune_anneal,
  search_space=space_glmnet
)


#train autotuner object
set.seed(1922)
with_progress(autotune_glmnet$train(task_credit))


#extract tuning results archive as data table
archive_glmnet <- as.data.table(autotune_glmnet$archive)
#best tune: alpha 0.1231044 lambda 0.04147188 logloss 0.5871771


#set graph learner parameters to best tuning results
learner_glmnet$param_set$values <- autotune_glmnet$learner$param_set$values

```


```{r TuneHyper, eval=FALSE}

#create transformation functions
k_square <- function(k) {
  k <- 2^k
  k
}

k_power10 <- function(k) {
  k <- 10^k
  k
}


#create tuning space, with trafo and budget arguments
space_svm = ps(
  classif.svm.cost=p_int(1, 6, trafo=k_power10, tags="budget"),
  classif.svm.gamma=p_int(-8, 2, trafo=k_square)
)


#create autotuner object
autotune_svm = AutoTuner$new(
  learner=learner_svm,
  resampling=resample_cv,
  measure=measure_log,
  terminator=term_none,
  tuner=tune_hyper2,
  search_space=space_svm
)

```


### Classifiers and best tunes found

#### Naive Bayes


```{r LearnerBayes}

#Naive Bayes classifier
learner_bayes <- as_learner(graph_preproc %>>% 
                              po("learner", 
                                 learner=lrn("classif.naive_bayes",
                                 predict_type="prob")))

```


#### glmnet


```{r LearnerGlmnet}

learner_glmnet <- as_learner(graph_preproc %>>% 
                               po("learner", 
                                  learner=lrn("classif.glmnet",
                                              predict_type="prob",
                                              alpha=0.1231044,
                                              lambda=0.04147188)))

```


#### kNN


```{r LearnerkNN}

learner_knn <- as_learner(graph_preproc %>>% 
                            po("learner", 
                              learner=lrn("classif.kknn",
                              predict_type="prob",
                              kernel="optimal",
                              scale=FALSE,
                              k=129,
                              distance=1.843605)))

```


#### SVM


```{r LearnerSVM}

#named class weights vector
svm_weight <- c(1, pipe_weights$param_set$values$minor_weight)
names(svm_weight) <- c("No", "Yes")


#graph learner
learner_svm <- as_learner(graph_preproc %>>% po("learner", 
                                                learner=lrn("classif.svm",
                                                            type="C-classification",
                                                            class.weights=svm_weight,
                                                            predict_type="prob",
                                                            kernel="radial",
                                                            scale=FALSE,
                                                            cost=10,
                                                            gamma=0.009167361)))

```


#### XGBoost

### Benchmarking

## Conclusions

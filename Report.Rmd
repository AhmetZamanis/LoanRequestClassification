---
title: "Imbalanced Classification - Loan request dataset"
author: "Ahmet Zamanis"
date: "2022-10-12"
output: 
  github_document:
    toc: yes
---

```{r setup, include=FALSE}

knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(message = FALSE)
knitr::opts_chunk$set(warning = FALSE)

options(scipen=999)

```

## Introduction

## Data preparation

```{r Packages}

library(tidyverse) #data handling & ggplot2
library(GGally) #exploratory analysis
library(DataExplorer) #exploratory analysis
library(patchwork) #combining ggplot2 objects
library(gt) #data tables
#library(ggedit) #editing ggplot2 objects
library(mlr3verse) #loads various mlr3 machine learning packages
library(mlr3hyperband) #hyperband tuning algorithm with mlr3
library(xgboost) #regularized gradient boosting

```


```{r LoadData}

#load and summarize original data
df <- read.csv("./OriginalData/clients.csv", header=TRUE)
summary(data.frame(unclass(df), stringsAsFactors = TRUE))

```


```{r RenameColumns}

#rename columns
names(df)[8] <- "have_children"
names(df)[13] <- "prev_client"
names(df)[14] <- "bad_client"

```


```{r RecodeEducation}

#recode levels of education column
df$education <- recode(df$education, "Higher education"="Higher",
                       "Incomplete higher education"="HigherDropout",
                       "Incomplete secondary education"="Secondary",
                       "PhD degree"="Higher",
                       "Secondary education"="Secondary",
                       "Secondary special education"="SecondarySpec")

```


```{r RecodeProductType}

#recode levels of product_type column, combining unfrequent levels together
#new levels: Beauty, Vehicles, Necessities
df$product_type <- recode(df$product_type, 
                          "Cosmetics and beauty services" = "Beauty",
                          "Jewelry" = "Beauty",
                          "Auto" = "Vehicles",
                          "Boats" = "Vehicles",
                          "Medical services" = "Necessities",
                          "Training" = "Necessities",
                          "Childen's goods" = "Necessities"
                          )


#new level: Recreational
lookup_recreational <- tibble(old=c("Tourism", "Music", "Fishing and hunting supplies",
                                    "Sporting goods", "Audio & Video", "Fitness"),
                              new=c(rep("Recreational", 6)))
df$product_type <- recode(df$product_type, !!!deframe(lookup_recreational))


#new level: Hardware
lookup_hardware <- tibble(old=c("Windows & Doors", "Construction Materials", "Repair Services",
                                "Garden equipment"),
                              new=c(rep("Hardware", 4)))
df$product_type <- recode(df$product_type, !!!deframe(lookup_hardware))


#shorten the level Household appliances
df$product_type <- recode(df$product_type, "Household appliances"="Appliances")

#remove intermediary objects
rm(prod_types, prod_types_rare, lookup_hardware, lookup_recreational)

```


```{r FactorConversions}

#factor convert categorical columns
#month, sex, product_type, family status, region, phone operator: 
  #factorize, don't change levels
for (i in c(1, 5, 7, 9, 11, 12)){
  df[,i] <- as.factor(df[,i])
}
rm(i)


#have children, region, phone_operator, prev_client, bad_client
  #factorize, recode levels
for (i in c(8, 13, 14)) {
  df[,i] <- recode(as.character(df[,i]),
                   "0" = "No",
                   "1" = "Yes")
  df[,i] <- as.factor(df[,i])
}
rm(i)


#education: factorize and order levels
df$education <- factor(df$education, levels=c("SecondarySpec", "Secondary",
                                              "HigherDropout", "Higher"),
                       ordered=TRUE)

```


```{r SummaryDataPrep}

summary(df)

```


## Exploratory analysis

### Numeric features

```{r EDANumerics, echo=FALSE}

ggpairs(df,
        columns=c("credit_amount", "credit_term", "age", "income", "bad_client"),
        mapping=(aes(color=bad_client, fill=bad_client,  alpha=0.2)),
        title="Numeric features: Distributions, correlations and relationships with outcome",
        progress=FALSE) +
  scale_color_manual(values=c("#00BFC4", "#F8766D")) +
  scale_fill_manual(values=c("#00BFC4", "#F8766D")) +
  theme_bw()

```


### Categorical features

```{r EDACat, echo=FALSE}

#acquire grouped dataframes with frequencies
df_bar1 <- df %>% group_by(month, bad_client) %>% summarise(count=n()) %>% mutate(freq=count/sum(count))
df_bar2 <- df %>% group_by(sex, bad_client) %>% summarise(count=n()) %>% mutate(freq=count/sum(count))
df_bar3 <- df %>% group_by(education, bad_client) %>% summarise(count=n()) %>% mutate(freq=count/sum(count))
df_bar4 <- df %>% group_by(product_type, bad_client) %>% summarise(count=n()) %>% mutate(freq=count/sum(count))
df_bar5 <- df %>% group_by(have_children, bad_client) %>% summarise(count=n()) %>% mutate(freq=count/sum(count))
df_bar6 <- df %>% group_by(region, bad_client) %>% summarise(count=n()) %>% mutate(freq=count/sum(count))
df_bar7 <- df %>% group_by(family_status, bad_client) %>% summarise(count=n()) %>% mutate(freq=count/sum(count))
df_bar8 <- df %>% group_by(phone_operator, bad_client) %>% summarise(count=n()) %>% mutate(freq=count/sum(count))
df_bar9 <- df %>% group_by(prev_client, bad_client) %>% summarise(count=n()) %>% mutate(freq=count/sum(count))


#put them in a list
list_bar <- list(df_bar1, df_bar2, df_bar3, df_bar4, df_bar5, df_bar6, df_bar7, df_bar8, df_bar9)


#loop to create plots
for (i in 1:9) {
  df_bar <- as.data.frame(list_bar[[i]])
  x_val <- df_bar[,4]
  y_val <- df_bar[,1]
  y_lab <- names(df_bar[1])
  group_val <- df_bar[,2]
  
  p <- ggplot(df_bar, aes(x=!!x_val, y=!!y_val, color=!!group_val, fill=!!group_val)) +
    geom_bar(stat="identity", position="stack", alpha=0.5, width=0.75) +
    labs(x="Frequency", y=y_lab, color="bad_client", fill="bad_client") +
    scale_color_manual(values=c("#00BFC4", "#F8766D")) +
    scale_fill_manual(values=c("#00BFC4", "#F8766D")) +
    theme_bw()
  
  assign(paste0("bar", i), p)
}
rm(df_bar, x_val, y_val, y_lab, group_val, p, i)

```


```{r, EDACat1, echo=FALSE}

(bar1 | bar2 | bar3) / (bar4 | bar5) + plot_layout(guides="collect") +
  plot_annotation(title="Categorical features, broken down by the outcome")

```


```{r, EDACat2, echo=FALSE}

(bar6 | bar7) / (bar8 | bar9) + plot_layout(guides="collect") +
  plot_annotation(title="Categorical features, broken down by the outcome")

```


## Feature engineering

### Creating new features


```{r FeatEng}

#payment: amount to be paid in 1 payment period
df <- df %>% mutate(payment = round(credit_amount / credit_term, 0), .after=credit_term)

#ratio_amount: total amount to be paid / income
df <- df %>% mutate(ratio_amount = round(credit_amount / income, 2), .after=credit_amount)

#ratio_payment: one payment / income
df <- df %>% mutate(ratio_payment = round(payment / income, 2), .after=payment)

```


### Evaluating new features

```{r FeatEngEval, echo=FALSE}

ggpairs(df,
        columns=c("payment", "ratio_amount", "ratio_payment", "bad_client"),
        mapping=(aes(color=bad_client, fill=bad_client, alpha=0.5)),
        title="New features and the outcome") +
        scale_color_manual(values=c("#00BFC4", "#F8766D")) +
        scale_fill_manual(values=c("#00BFC4", "#F8766D")) +
  theme_bw()

```


## Modeling

### Preprocessing

```{r OrdinalEncode}

encode_ordinal <- function(x, order = unique(x)) {
  x <- as.numeric(factor(x, levels = order, exclude = NULL)) - 1
  x
}

df$education <- encode_ordinal(df$education, order=levels(df$education))

```


```{r PipelinePreproc}

#create preprocessing pipeline

#yeo-johnson transformation for numeric features
pipe_yeo <- po("yeojohnson", standardize=FALSE)


#weight of evidence encoding for nominal features
encode_woe <- po("encodeimpact")


#centering and scaling for numeric features
pipe_center_scale <- po("scale")


#add interaction terms with model matrix
pipe_interact <- po("modelmatrix", formula = ~ . + credit_amount:product_type.No +
                      credit_amount:product_type.Yes +
                      age:income + age:family_status.No + age:family_status.Yes +
                      age:have_children.No + age:have_children.Yes)


#add minority class weights
pipe_weights <- po("classweights")
pipe_weights$param_set$values$minor_weight <- summary(df$bad_client)[1] /
  summary(df$bad_client)[2]


#create combined preprocessing pipeline (Graph in mlr3)
graph_preproc <- pipe_yeo %>>% encode_woe %>>% pipe_center_scale %>>%
  pipe_interact %>>% pipe_weights

```


```{r mlr3Objects}

#create classification task
task_credit <- as_task_classif(df, target="bad_client", positive="Yes")


#create 5-folds resampling
resample_cv <- rsmp("cv", folds=5L)


#create performance measures
source("mlr3CustomMeasures.R") #source custom implementation of Cohen's kappa measure
measure_rec <- msr("classif.recall") 
measure_prauc <- msr("classif.prauc") 
measure_brier <- msr("classif.bbrier") 
measure_kappa <- msr("classif.kappa") 
measure_acc <- msr("classif.acc") 
measure_time <- msr("time_both")
measures_list <- c(measure_rec, measure_prauc, measure_brier, measure_kappa, 
                   measure_acc, measure_time)


#logloss, just as a tuning metric
measure_log <- msr("classif.logloss")


#create tuners
tune_grid <- mlr3tuning::tnr("grid_search")
tune_random <- mlr3tuning::tnr("random_search") 
tune_hyper2 <- mlr3tuning::tnr("hyperband", eta=2) 
tune_hyper3 <- mlr3tuning::tnr("hyperband", eta=3) 
tune_anneal <- mlr3tuning::tnr("gensa") 


#create tuning terminators
term10 <- trm("evals", n_evals=10)
term25 <- trm("evals", n_evals=25)
term50 <- trm("evals", n_evals=50)
term100 <- trm("evals", n_evals=100)
term250 <- trm("evals", n_evals=250)
term500 <- trm("evals", n_evals=500)
term_none <- trm("none")

```


### Approach to hyperparameter tuning

```{r TuneAnneal, eval=FALSE}

#create preprocessing + model pipeline (GraphLearner in mlr3)
learner_glmnet <- as_learner(graph_preproc %>>% po("learner", 
                                                   learner=lrn("classif.glmnet",
                                                   predict_type="prob")))


#create a search space for tuning
space_glmnet <- ps(
  classif.glmnet.alpha=p_dbl(lower=0, upper=1),
  classif.glmnet.lambda=p_dbl(lower=0, upper=1)
)


#create autotuner object
autotune_glmnet = AutoTuner$new(
  learner=learner_glmnet,
  resampling=resample_cv,
  measure=measure_log,
  terminator=term100,
  tuner=tune_anneal,
  search_space=space_glmnet
)


#train autotuner object
set.seed(1922)
with_progress(autotune_glmnet$train(task_credit))


#extract tuning results archive as data table
archive_glmnet <- as.data.table(autotune_glmnet$archive)
#best tune: alpha 0.1231044 lambda 0.04147188 logloss 0.5871771


#set graph learner parameters to best tuning results
learner_glmnet$param_set$values <- autotune_glmnet$learner$param_set$values

```


```{r TuneHyper, eval=FALSE}

#create transformation functions
k_square <- function(k) {
  k <- 2^k
  k
}

k_power10 <- function(k) {
  k <- 10^k
  k
}


#create tuning space, with trafo and budget arguments
space_svm = ps(
  classif.svm.cost=p_int(1, 6, trafo=k_power10, tags="budget"),
  classif.svm.gamma=p_int(-8, 2, trafo=k_square)
)


#create autotuner object
autotune_svm = AutoTuner$new(
  learner=learner_svm,
  resampling=resample_cv,
  measure=measure_log,
  terminator=term_none,
  tuner=tune_hyper2,
  search_space=space_svm
)

```


### Classifiers and best tunes found

#### Naive Bayes


```{r LearnerBayes}

#Naive Bayes graph learner
learner_bayes <- as_learner(graph_preproc %>>% 
                              po("learner", 
                                 learner=lrn("classif.naive_bayes",
                                 predict_type="prob")))

```


#### glmnet


```{r LearnerGlmnet}

#glmnet graph learner
learner_glmnet <- as_learner(graph_preproc %>>% 
                               po("learner", 
                                  learner=lrn("classif.glmnet",
                                              predict_type="prob",
                                              alpha=0.1231044,
                                              lambda=0.04147188)))

```


#### kNN


```{r LearnerkNN}

#kNN graph learner
learner_knn <- as_learner(graph_preproc %>>% 
                            po("learner", 
                              learner=lrn("classif.kknn",
                              predict_type="prob",
                              kernel="optimal",
                              scale=FALSE,
                              k=129,
                              distance=1.843605)))

```


#### SVM


```{r LearnerSVM}

#named class weights vector
svm_weight <- c(1, pipe_weights$param_set$values$minor_weight)
names(svm_weight) <- c("No", "Yes")


#SVM graph learner
learner_svm <- as_learner(graph_preproc %>>% po("learner", 
                                                learner=lrn("classif.svm",
                                                            type="C-classification",
                                                            class.weights=svm_weight,
                                                            predict_type="prob",
                                                            kernel="radial",
                                                            scale=FALSE,
                                                            cost=10,
                                                            gamma=0.009167361)))

```


#### XGBoost


```{r}

#XGBoost graph learner
learner_xgb <- as_learner(graph_preproc %>>% po("learner", 
                                                learner=lrn("classif.xgboost",
                                                            booster="gbtree",
                                                            predict_type="prob",
                                                            nthread=3,
                                                            nrounds=27,
                                                            eta=0.06444704,
                                                            max_depth=12,
                                                            min_child_weight=20,
                                                            max_delta_step=8,
                                                            gamma=0.73399971,
                                                            lambda=0.9294227,
                                                            alpha=0.14594723)))


```


### Benchmarking


```{r Benchmark}

#create baseline learner that predicts the class probabilities as the class frequencies
learner_baseline <- as_learner(graph_preproc %>>% 
                                 po("learner", 
                                    learner=lrn("classif.featureless",
                                    predict_type="prob",
                                    method="weighted.sample")))

  
#create benchmark grid
benchmark_test = benchmark_grid(tasks=task_credit,
                                   learn=list(learner_baseline, learner_bayes, 
                                              learner_glmnet,
                                              learner_knn, learner_svm,
                                              learner_xgb),
                                   resamplings=resample_cv)


#perform benchmarking
set.seed(1923)
benchmarkres = benchmark(benchmark_test, store_models=TRUE)


#save average benchmarking results
benchmarkres_table <- benchmarkres$aggregate(measures_list)

```


```{r MetricsTable, echo=FALSE}

#dataframe of metrics
df_metrics <- benchmarkres_table[,c(4, 7:12)]
colnames(df_metrics) <- c("Classifier", "Recall", "PRAUC",
                              "Brier score", "Cohen's kappa", "Accuracy", "Time")
df_metrics[1:6, 1] <- c("Baseline", "NaiveBayes", "glmnet", "kNN", "SVM", "XGBoost")
df_metrics[,2:6] <- round(df_metrics[,2:6], 4)


#gt table
tb_perf <- gt(data=df_metrics, rowname_col = 1) %>% 
  tab_header(title="Performance metrics of loan client classifiers",
             subtitle="Averages of 5-folds crossvalidation. Threshold probability=0.5") %>% 
  opt_table_font(font=list(google_font("Calibri"), default_fonts())) %>% 
  cols_align(align = "center", columns = everything()) %>% 
  tab_style(locations=cells_column_labels(columns=everything()), 
            style=list(
              cell_borders(sides="bottom", weight=px(3)), 
              cell_text(weight="bold"))) %>% 
  data_color(columns=c(2,3,5,6), colors=scales::col_numeric(
    palette=c("red", "green"),
    domain=c(-0.0023,1)
  )) %>%
  data_color(columns=c(4), colors=scales::col_numeric(
    palette=c("green", "red"),
    domain=c(0,2)
    )) %>%
      data_color(columns=c(7), colors=scales::col_numeric(
        palette=c("green", "red"),
        domain=c(0,max(df_metrics[,7]))
        ))


#call table
tb_perf

```



```{r PRCCurves, echo=FALSE}

#attach ggedit library (not before because it breaks ggpairs)
library(ggedit)

#make prc plot with mlr3 autoplot, modify resulting ggplot2 object
prc_plot <- mlr3viz::autoplot(benchmarkres, type="prc")
prc_plot <- prc_plot + 
  geom_line(aes(x=x, y=y, color=modname)) +
  #geom_ribbon(aes(x=x, y=y, ymin=0, ymax=y, fill=modname), alpha=0.075) +
  labs(x="Recall", y="Precision",
       title="Precision-recall curves of loan client classifiers",
       subtitle="Baseline classifier predicts the class frequencies as the probability of each class",
       color="Classifiers",
       fill="Classifiers") + 
  theme(legend.position = "right") + 
  scale_color_brewer(palette="Dark2", labels=c("Baseline", "NaiveBayes", "glmnet", "kNN", "SVM", "XGBoost")) + 
  #scale_fill_brewer(palette="Dark2", labels=c("Baseline", "NaiveBayes", "glmnet", "kNN", "SVM", "XGBoost")) +
  theme_bw()

#line size
prc_plot$layers[[2]]$aes_params$size=1

#supress confidence bands
prc_plot <- remove_geom(p=prc_plot, geom="smooth")

#call plot
prc_plot

```


## Conclusions

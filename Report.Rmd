---
title: "Imbalanced Classification - Loan request dataset"
author: "Ahmet Zamanis"
date: "2022-10-12"
output: 
  github_document:
    toc: yes
---

```{r setup, include=FALSE}

knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(message = FALSE)
knitr::opts_chunk$set(warning = FALSE)

options(scipen=999)

```

## Introduction

A common challenge in machine learning problems is classification with class imbalanced data. Class imbalance is when the target variable we want to classify has a very imbalanced class distribution, with the minority class(es) making up a small fraction of all observations. A typical example is detecting loan defaulters or credit card fraudsters, as these will naturally make up a small minority of all customers. With a class imbalanced problem, it is key to assess the performance of ML models with the correct metrics. Basic metrics such as accuracy can be very misleading. 
\
\
In this analysis, we will use a [dataset](https://www.kaggle.com/datasets/podsyp/is-this-a-good-customer) of client loan requests, and try to classify the clients / loans into "bad" or "good" binary classes, based on features like income, education and loan category. We will try several of the most established classification algorithms, evaluate them with several performance metrics, and use class weighting to try and improve performance.

## Data preparation

```{r Packages}

library(tidyverse) #data handling & ggplot2
library(GGally) #exploratory analysis
library(patchwork) #combining ggplot2 objects
library(gt) #data tables
#library(ggedit) #editing ggplot2 objects (load later as it conflicts with GGally)
library(mlr3verse) #mlr3 machine learning packages
library(mlr3hyperband) #hyperband tuning algorithm with mlr3
library(xgboost) #regularized gradient boosting

```

Let's load our original dataset, and examine the columns.
```{r LoadData}

#load and summarize original data
df <- read.csv("./OriginalData/clients.csv", header=TRUE)
summary(data.frame(unclass(df), stringsAsFactors = TRUE))

```
Our target column is bad_client_target, a binary categorical variable. 0 stands for "good" and 1 for "bad" client. A better way to think about it is "good loan request" and "bad loan request", as some of the features are specific to the loan request.

  - Our categorical features are month (of the loan request), sex, education level, product type (loan category), whether the client has children, region, family status, the client's phone operator and whether the client is a previous client.
  - Our numeric features are requested credit amount (in total), credit terms (apparently in months), age and income (presumably yearly). 

First, we will rename some of the column names, and recode some of the categorical features' levels, for ease of coding and plotting.
```{r RenameColumns}

#rename columns
names(df)[8] <- "have_children"
names(df)[13] <- "prev_client"
names(df)[14] <- "bad_client"

```

The product type column has high cardinality, with numerous levels, some of them very infrequent with less than 10 observations. Estimates for very infrequent categorical levels may be biased due to lack of observations. We will recode the infrequent categories into more general ones, as sensibly as we can. The education column also has one infrequent level, "PhD degree" which we'll also recode into "Higher" education.
```{r RecodeEducation}

#recode levels of education column
df$education <- recode(df$education, "Higher education"="Higher",
                       "Incomplete higher education"="HigherDropout",
                       "Incomplete secondary education"="Secondary",
                       "PhD degree"="Higher",
                       "Secondary education"="Secondary",
                       "Secondary special education"="SecondarySpec")

```


```{r RecodeProductType}

#recode levels of product_type column, combining infrequent levels together
#new levels: Beauty, Vehicles, Necessities
df$product_type <- recode(df$product_type, 
                          "Cosmetics and beauty services" = "Beauty",
                          "Jewelry" = "Beauty",
                          "Auto" = "Vehicles",
                          "Boats" = "Vehicles",
                          "Medical services" = "Necessities",
                          "Training" = "Necessities",
                          "Childen's goods" = "Necessities"
                          )


#new level: Recreational
lookup_recreational <- tibble(old=c("Tourism", "Music", "Fishing and hunting supplies",
                                    "Sporting goods", "Audio & Video", "Fitness"),
                              new=c(rep("Recreational", 6)))
df$product_type <- recode(df$product_type, !!!deframe(lookup_recreational))


#new level: Hardware
lookup_hardware <- tibble(old=c("Windows & Doors", "Construction Materials", "Repair Services",
                                "Garden equipment"),
                              new=c(rep("Hardware", 4)))
df$product_type <- recode(df$product_type, !!!deframe(lookup_hardware))


#shorten the level Household appliances
df$product_type <- recode(df$product_type, "Household appliances"="Appliances")

#remove intermediary objects
rm(prod_types, prod_types_rare, lookup_hardware, lookup_recreational)

```


We'll convert our categorical features into factors, to be used in plotting, and later to be encoded in preprocessing. Our only ordinal feature is education, and we'll order its levels according to the levels of education.
```{r FactorConversions}

#factor convert categorical columns
#month, sex, product_type, family status, region, phone operator: 
  #factorize, don't change levels
for (i in c(1, 5, 7, 9, 11, 12)){
  df[,i] <- as.factor(df[,i])
}
rm(i)


#have children, region, phone_operator, prev_client, bad_client
  #factorize, recode levels
for (i in c(8, 13, 14)) {
  df[,i] <- recode(as.character(df[,i]),
                   "0" = "No",
                   "1" = "Yes")
  df[,i] <- as.factor(df[,i])
}
rm(i)


#education: factorize and order levels
df$education <- factor(df$education, levels=c("SecondarySpec", "Secondary",
                                              "HigherDropout", "Higher"),
                       ordered=TRUE)

```


Let's see how the data looks with these changes.
```{r SummaryDataPrep}

summary(df)

```


## Exploratory analysis

We'll perform exploratory analysis on the original features before we engineer new features.

### Numeric features

Let's start with our numeric features, plotting their distributions, correlations with one another, and their relationships with the outcome.
```{r EDANumerics, echo=FALSE}

ggpairs(df,
        columns=c("credit_amount", "credit_term", "age", "income", "bad_client"),
        mapping=(aes(color=bad_client, fill=bad_client,  alpha=0.2)),
        title="Numeric features: Distributions, correlations and relationships with outcome",
        progress=FALSE) +
  scale_color_manual(values=c("#00BFC4", "#F8766D")) +
  scale_fill_manual(values=c("#00BFC4", "#F8766D")) +
  theme_bw()

```
\
From the histograms and density plots, we see our numeric features are not normally distributed, but very right-tailed.

  - There is no strong correlation between the numeric features. However, it's notable that the correlation of income with credit_term and age are higher for "bad" clients, though still low overall.
  - Looking at the boxplots and density plots, we see our numeric features except credit_term are likely not good predictors of the outcome.
    - It seems bad loan clients are generally more likely to request longer credit terms.
    - Bad loan clients tend to be a little younger.
    - The absolute highest credit amounts seem to be awarded to good clients, and the highest incomes belong to good clients, but overall there is little difference between good and bad clients in terms of credit amount and income.

### Categorical features

```{r EDACat, echo=FALSE}

#acquire grouped dataframes with frequencies
df_bar1 <- df %>% group_by(month, bad_client) %>% summarise(count=n()) %>% mutate(freq=count/sum(count))
df_bar2 <- df %>% group_by(sex, bad_client) %>% summarise(count=n()) %>% mutate(freq=count/sum(count))
df_bar3 <- df %>% group_by(education, bad_client) %>% summarise(count=n()) %>% mutate(freq=count/sum(count))
df_bar4 <- df %>% group_by(product_type, bad_client) %>% summarise(count=n()) %>% mutate(freq=count/sum(count))
df_bar5 <- df %>% group_by(have_children, bad_client) %>% summarise(count=n()) %>% mutate(freq=count/sum(count))
df_bar6 <- df %>% group_by(region, bad_client) %>% summarise(count=n()) %>% mutate(freq=count/sum(count))
df_bar7 <- df %>% group_by(family_status, bad_client) %>% summarise(count=n()) %>% mutate(freq=count/sum(count))
df_bar8 <- df %>% group_by(phone_operator, bad_client) %>% summarise(count=n()) %>% mutate(freq=count/sum(count))
df_bar9 <- df %>% group_by(prev_client, bad_client) %>% summarise(count=n()) %>% mutate(freq=count/sum(count))


#put them in a list
list_bar <- list(df_bar1, df_bar2, df_bar3, df_bar4, df_bar5, df_bar6, df_bar7, df_bar8, df_bar9)


#loop to create plots
for (i in 1:9) {
  df_bar <- as.data.frame(list_bar[[i]])
  x_val <- df_bar[,4]
  y_val <- df_bar[,1]
  y_lab <- names(df_bar[1])
  group_val <- df_bar[,2]
  
  p <- ggplot(df_bar, aes(x=!!x_val, y=!!y_val, color=!!group_val, fill=!!group_val)) +
    geom_bar(stat="identity", position="stack", alpha=0.5, width=0.75) +
    labs(x="Frequency", y=y_lab, color="bad_client", fill="bad_client") +
    scale_color_manual(values=c("#00BFC4", "#F8766D")) +
    scale_fill_manual(values=c("#00BFC4", "#F8766D")) +
    theme_bw()
  
  assign(paste0("bar", i), p)
}
rm(df_bar, x_val, y_val, y_lab, group_val, p, i)

```

We'll evaluate the relationships of our categorical features by making mosaic plots, with each level of the categorical feature colored by the outcome. Each bar represents the proportion of good and bad clients in that level of the categorical feature.
```{r, EDACat1, echo=FALSE}

(bar1 | bar2 | bar3) / (bar4 | bar5) + plot_layout(guides="collect") +
  plot_annotation(title="Categorical features, broken down by the outcome")

```

\
Overall, our first five categorical features seem to be relevant predictors, though to different degrees.

  - Product type / loan category seems to be a significant predictor, as the proportion of bad clients change considerably depending on its levels.
    - Clients that request loans for cell phone purchases are almost 25% bad clients, while the proportion is close to zero for some other categories. 
    - Another particularly suspect category is Beauty, which consists of loans for cosmetics, beauty services and jewelry.
  - There seems to be some seasonality, as bad clients make very few loan requests in July, and considerably more in December, likely for holiday shopping.
  - Bad clients are more likely to have secondary level education, and considerably less likely to have higher education.
  - Female clients, or clients without children, are a bit more likely to make bad loan requests. The effect of not having children may actually be caused by age, as clients without children are likely to be younger.

```{r, EDACat2, echo=FALSE}

(bar6 | bar7) / (bar8 | bar9) + plot_layout(guides="collect") +
  plot_annotation(title="Categorical features, broken down by the outcome")

```
\
Previous clients seem to be more likely to make bad loan requests.

  - The meaning of the family status level "Another" is unclear, even though it makes up the majority of the observations. Still, "Another" and "Unmarried" clients are less likely to make bad requests, while married clients are more likely.
  - Clients from region 2 are more likely to make bad loan requests, and those from region 1 are the least likely.
  - Clients who use phone operators 0 and 1 are more likely to make bad requests, and users of operator 2 are least likely.

## Feature engineering

The performance of machine learning algorithms greatly depend on the quality and predictiveness of the features. If the features are not predictive, using more sophisticated models and fine-tuning hyperparameters likely won't bring much improvement.
\
\
Until now, it looks like the features in our dataset are not particularly predictive of our target variable. Some of our categorical features seem to be decently predictive of bad loan clients, while the numerics tend to show weak relationships, if any. Let's see if we can extract better features from our data.

### Creating new features

We have the total amounts, and number of payment installments requested for the loans. We can calculate the amounts to be paid in one installment as a possibly significant feature. We can also compute the ratio of the loan amount to the client's income, and the ratio of one payment to the client's income, as possible indicators of their solvency. 
```{r FeatEng}

#payment: amount to be paid in 1 payment period
df <- df %>% mutate(payment = round(credit_amount / credit_term, 0), .after=credit_term)

#ratio_amount: total amount to be paid / income
df <- df %>% mutate(ratio_amount = round(credit_amount / income, 2), .after=credit_amount)

#ratio_payment: one payment / income
df <- df %>% mutate(ratio_payment = round(payment / income, 2), .after=payment)

```
Different machine learning algorithms may benefit differently from feature engineering. 

  - For example, a linear model can't evaluate relationships such as ratios or interactions unless we specifically give them as inputs, so it is more likely to benefit from the features we added. 
  - On the other hand, tree-based models split the data consecutively with different combinations of features, so they are able to discover relationships between features, though they may still benefit from having them as direct inputs.
  
We will add more interaction terms in the preprocessing steps.

### Evaluating new features

All of our engineered features are numerics, so let's evaluate them the same way.
```{r FeatEngEval, echo=FALSE}

ggpairs(df,
        columns=c("payment", "ratio_amount", "ratio_payment", "bad_client"),
        mapping=(aes(color=bad_client, fill=bad_client, alpha=0.5)),
        title="New features and the outcome") +
        scale_color_manual(values=c("#00BFC4", "#F8766D")) +
        scale_fill_manual(values=c("#00BFC4", "#F8766D")) +
  theme_bw()

```
\
Our new features are even more right-tailed compared to the original numeric features. We'd do well to normalize them in preprocessing, especially for the sake of our Naive Bayes classifier.

  - The new features do not seem particularly predictive of bad loan clients either.
    - The highest values for one loan installment, ratio of one installment to income, and ratio of total loan amount to income all belong exclusively to good loan clients. These are likely wealthy clients with considerable non-income assets, who are able to take on the biggest loans with collateral. However, this doesn't directly help us identify bad loan clients.  
  - The ratio of the total loan amount to income, and the ratio of one payment to income are highly correlated, which is expected.
    - Some of the classifiers we'll use are more susceptible to multicollinearity, while others can tackle it better with regularization. 
    - We'll keep the ratio features as they are, as they seem somewhat predictive. We could apply principal components analysis to break them up into uncorrelated PCs, but these are not guaranteed to be predictive like the original features.

## Modeling

We'll use the [mlr3 packages](https://mlr3.mlr-org.com/) to apply preprocessing steps, classification models, resampling methods and performance benchmarking. The use of pipelines will ensure preprocessing steps are correctly applied while keeping the implementation and code much more cleaner than a manual implementation.

### Preprocessing

We have many categorical features, and we need to convert them into numerics for many ML algorithms. First, we can apply ordinal encoding to the education feature manually quite easily. We don't need to do this separately for training and testing data, as we'd just get the same results. 
```{r OrdinalEncode}

#function to perform ordinal encoding on a column
encode_ordinal <- function(x, order = unique(x)) {
  x <- as.numeric(factor(x, levels = order, exclude = NULL)) - 1
  x
}

df$education <- encode_ordinal(df$education, order=levels(df$education))

```
Ordinal encoding is not suitable for the rest of our categorical features, which are nominal with no order between their levels.
\
\
The rest of our preprocessing will be carried out with pipe operators combined in a pipeline, or a graph as it is called in mlr3. The preprocessing steps, in order are:

  - Normalization of the numeric features, with a Yeo-Johnson transformation. This is a different formulation of the better known Box-Cox transformation that also handles negative values.
  - Target encoding of nominal categoricals, specifically with the weight of evidence method.
    - Target encoding methods use information from the target variable to encode levels of the categorical features.
    - Weight of evidence target encoding uses the log-likelihoods of the target variable levels, given the levels of the categorical feature, to replace the categorical values with numeric values. It is commonly used in credit risk scoring.
  - Centering and scaling of numeric features.
  - Adding interaction terms to the model matrix.
   - Interaction terms are best calculated from centered & scaled numeric features, but shouldn't be centered & scaled again afterwards.
  - Adding minority class weights to classifiers that accept this parameter. 
    - Class weights determine which observations in the dataset get a bigger weight in the classifier's scoring function. Errors for observations with higher class weight are penalized more heavily. 
    - The default is a weight of 1 for each observation. We can assign a larger class weight to the minority class observations, bad clients, so the model is more sensitive to mistakes for the minority class.
    - A typical value for the minority class weight is calculated as (N. majority class obs. / N. minority class obs.).

We'll combine these preprocessing steps into a "graph", a pipeline that will apply these steps in order every time we train a model. This will ensure they are applied in the correct order. Also, the calculations for some of these steps, especially target encoding, should only be derived from the training data, to avoid leakage from the testing data into model training. This is ensured by the pipeline approach.
```{r PipelinePreproc}

#create preprocessing pipeline

#yeo-johnson transformation
pipe_yeo <- po("yeojohnson", standardize=FALSE)


#weight of evidence encoding
encode_woe <- po("encodeimpact")


#centering and scaling
pipe_center_scale <- po("scale")


#adding interaction terms with model matrix
pipe_interact <- po("modelmatrix", formula = ~ . + credit_amount:product_type.No +
                      credit_amount:product_type.Yes +
                      age:income + age:family_status.No + age:family_status.Yes +
                      age:have_children.No + age:have_children.Yes)


#adding minority class weights
pipe_weights <- po("classweights")
pipe_weights$param_set$values$minor_weight <- summary(df$bad_client)[1] /
  summary(df$bad_client)[2]


#create combined preprocessing pipeline (called Graph in mlr3)
graph_preproc <- pipe_yeo %>>% encode_woe %>>% pipe_center_scale %>>%
  pipe_interact %>>% pipe_weights

```
Before moving on, let's discuss the interaction terms we added to our model matrix:

  - Total credit amount X product type. This is likely an important interaction, as certain loan categories such as vehicles naturally warrant higher amounts, while large requests for some other categories may be an indicator of a bad request.
  - Age X having children, and Age X family status. It's likely the effects of being married and having children are dependent on age, as younger couples and parents may be less resilient financially.
  - Age X income. It's possible the effect of age is in turn really caused by income, as younger people generally, but not always, tend to have lower income.
  
Let's create the remaining mlr3 objects we need to perform our tuning and benchmarking:

  - A classification task, with our dataset, specify the target column, and the positive class.
  - A resampling object for 5-folds crossvalidation. 
    - Our preprocessing pipeline will ensure the preprocessing steps are reapplied to every training set during resampling, avoiding leakage from the testing data.
  - Numerous performance metrics to tune and benchmark our models. We'll discuss them in detail in the tuning and benchmarking sections. 
    - The Cohen's Kappa metric is not normally available in mlr3measures, but it's straightforward to [add custom measures](https://mlr3book.mlr-org.com/extending.html). I [added](https://github.com/AhmetZamanis/LoanRequestClassification/blob/main/mlr3CustomMeasures.R) Kappa and several other classification metrics, which are calculated by calling the package [yardstick](https://yardstick.tidymodels.org/).
  - Tuners with numerous hyperparameter tuning algorithms, and terminators to determine how many evaluations will be made. We'll discuss them more in the hyperparameter tuning section.

```{r mlr3Objects}

#create classification task
task_credit <- as_task_classif(df, target="bad_client", positive="Yes")


#create 5-folds resampling
resample_cv <- rsmp("cv", folds=5L)


#create performance measures
source("mlr3CustomMeasures.R") #source custom implementation of Cohen's kappa measure
measure_rec <- msr("classif.recall") 
measure_prauc <- msr("classif.prauc") 
measure_brier <- msr("classif.bbrier") 
measure_kappa <- msr("classif.kappa") 
measure_acc <- msr("classif.acc") 
measure_time <- msr("time_both")
measures_list <- c(measure_rec, measure_prauc, measure_brier, measure_kappa, 
                   measure_acc, measure_time)


#create logloss measure, just as a tuning metric
measure_log <- msr("classif.logloss")


#create tuners
tune_grid <- mlr3tuning::tnr("grid_search")
tune_random <- mlr3tuning::tnr("random_search") 
tune_hyper2 <- mlr3tuning::tnr("hyperband", eta=2) 
tune_hyper3 <- mlr3tuning::tnr("hyperband", eta=3) 
tune_anneal <- mlr3tuning::tnr("gensa") 


#create tuning terminators
term10 <- trm("evals", n_evals=10)
term25 <- trm("evals", n_evals=25)
term50 <- trm("evals", n_evals=50)
term100 <- trm("evals", n_evals=100)
term250 <- trm("evals", n_evals=250)
term500 <- trm("evals", n_evals=500)
term_none <- trm("none")

```


### Approach to hyperparameter tuning

```{r TuneAnneal, eval=FALSE}

#create preprocessing + model pipeline (GraphLearner in mlr3)
learner_glmnet <- as_learner(graph_preproc %>>% po("learner", 
                                                   learner=lrn("classif.glmnet",
                                                   predict_type="prob")))


#create a search space for tuning
space_glmnet <- ps(
  classif.glmnet.alpha=p_dbl(lower=0, upper=1),
  classif.glmnet.lambda=p_dbl(lower=0, upper=1)
)


#create autotuner object
set.seed(1922)
autotune_glmnet = AutoTuner$new(
  learner=learner_glmnet,
  resampling=resample_cv,
  measure=measure_log,
  terminator=term100,
  tuner=tune_anneal,
  search_space=space_glmnet
)


#train autotuner object on the classification task
set.seed(1922)
with_progress(autotune_glmnet$train(task_credit))


#extract tuning results archive as data table
archive_glmnet <- as.data.table(autotune_glmnet$archive)
#best tune: alpha 0.1231044 lambda 0.04147188 logloss 0.5871771


#set graph learner parameters to best tune from autotuner
learner_glmnet$param_set$values <- autotune_glmnet$learner$param_set$values

```


```{r TuneHyper, eval=FALSE}

#create transformation functions
k_square <- function(k) {
  k <- 2^k
  k
}

k_power10 <- function(k) {
  k <- 10^k
  k
}


#create tuning space, with trafo and budget arguments
space_svm = ps(
  classif.svm.cost=p_int(1, 6, trafo=k_power10, tags="budget"),
  classif.svm.gamma=p_int(-8, 2, trafo=k_square)
)


#create autotuner object
set.seed(1922)
autotune_svm = AutoTuner$new(
  learner=learner_svm,
  resampling=resample_cv,
  measure=measure_log,
  terminator=term_none,
  tuner=tune_hyper2,
  search_space=space_svm
)

```


### Classifiers, with the best tunes found

#### Naive Bayes


```{r LearnerBayes}

#Naive Bayes graph learner
learner_bayes <- as_learner(graph_preproc %>>% 
                              po("learner", 
                                 learner=lrn("classif.naive_bayes",
                                 predict_type="prob")))

```


#### glmnet


```{r LearnerGlmnet}

#glmnet graph learner
learner_glmnet <- as_learner(graph_preproc %>>% 
                               po("learner", 
                                  learner=lrn("classif.glmnet",
                                              predict_type="prob",
                                              alpha=0.1231044,
                                              lambda=0.04147188)))

```


#### kNN


```{r LearnerkNN}

#kNN graph learner
learner_knn <- as_learner(graph_preproc %>>% 
                            po("learner", 
                              learner=lrn("classif.kknn",
                              predict_type="prob",
                              kernel="optimal",
                              scale=FALSE,
                              k=129,
                              distance=1.843605)))

```


#### SVM


```{r LearnerSVM}

#named class weights vector
svm_weight <- c(1, pipe_weights$param_set$values$minor_weight)
names(svm_weight) <- c("No", "Yes")


#SVM graph learner
learner_svm <- as_learner(graph_preproc %>>% po("learner", 
                                                learner=lrn("classif.svm",
                                                            type="C-classification",
                                                            class.weights=svm_weight,
                                                            predict_type="prob",
                                                            kernel="radial",
                                                            scale=FALSE,
                                                            cost=10,
                                                            gamma=0.009167361)))

```


#### XGBoost


```{r, LearnerXGBoost}

#XGBoost graph learner
learner_xgb <- as_learner(graph_preproc %>>% po("learner", 
                                                learner=lrn("classif.xgboost",
                                                            booster="gbtree",
                                                            predict_type="prob",
                                                            nthread=3,
                                                            nrounds=27,
                                                            eta=0.06444704,
                                                            max_depth=12,
                                                            min_child_weight=20,
                                                            max_delta_step=8,
                                                            gamma=0.73399971,
                                                            lambda=0.9294227,
                                                            alpha=0.14594723)))


```


### Benchmarking


```{r Benchmark, results="hide"}

#create baseline learner
learner_baseline <- as_learner(graph_preproc %>>% 
                                 po("learner", 
                                    learner=lrn("classif.featureless",
                                    predict_type="prob",
                                    method="weighted.sample")))

  
#create benchmark grid
set.seed(1923)
benchmark_test = benchmark_grid(tasks=task_credit,
                                   learn=list(learner_baseline, learner_bayes, 
                                              learner_glmnet,
                                              learner_knn, learner_svm,
                                              learner_xgb),
                                   resamplings=resample_cv)


#perform benchmarking
set.seed(1923)
benchmarkres = benchmark(benchmark_test, store_models=TRUE)


#save average benchmarking results
benchmarkres_table <- benchmarkres$aggregate(measures_list)

```


```{r MetricsTable, echo=FALSE, out.width="75%", out.height="75%"}

#dataframe of metrics
df_metrics <- benchmarkres_table[,c(4, 7:12)]
colnames(df_metrics) <- c("Classifier", "Recall", "PRAUC",
                              "Brier score", "Cohen's kappa", "Accuracy", "Time")
df_metrics[1:6, 1] <- c("Baseline", "NaiveBayes", "glmnet", "kNN", "SVM", "XGBoost")
df_metrics[,2:6] <- round(df_metrics[,2:6], 4)


#gt table, save as png
perf_tb <- gt(data=df_metrics, rowname_col = 1) %>% 
  tab_header(title="Performance metrics of loan client classifiers",
             subtitle="Averages of 5-folds crossvalidation. Threshold probability=0.5") %>% 
  opt_table_font(font=list(google_font("Calibri"), default_fonts())) %>% 
  cols_align(align = "center", columns = everything()) %>% 
  tab_style(locations=cells_column_labels(columns=everything()), 
            style=list(
              cell_borders(sides="bottom", weight=px(3)), 
              cell_text(weight="bold"))) %>% 
  data_color(columns=c(2,3,5,6), colors=scales::col_numeric(
    palette=c("red", "green"),
    domain=c(-0.0023,1)
  )) %>%
  data_color(columns=c(4), colors=scales::col_numeric(
    palette=c("green", "red"),
    domain=c(0,2)
    )) %>%
      data_color(columns=c(7), colors=scales::col_numeric(
        palette=c("green", "red"),
        domain=c(0,max(df_metrics[,7]))
        ))
  
gtsave(data=perf_tb, filename="./Report_files/figure-gfm/MetricTable.png")

#load saved png
knitr::include_graphics("./Report_files/figure-gfm/MetricTable.png", dpi=NA)

```



```{r PRCCurves, echo=FALSE}

#attach ggedit library (not before because it breaks ggpairs)
library(ggedit)

#make prc plot with mlr3 autoplot, modify resulting ggplot2 object
prc_plot <- mlr3viz::autoplot(benchmarkres, type="prc")
prc_plot <- prc_plot + 
  geom_line(aes(x=x, y=y, color=modname)) +
  #geom_ribbon(aes(x=x, y=y, ymin=0, ymax=y, fill=modname), alpha=0.075) +
  labs(x="Recall", y="Precision",
       title="Precision-recall curves of loan client classifiers",
       subtitle="Baseline classifier predicts the class frequencies as the probability of each class",
       color="Classifiers",
       fill="Classifiers") + 
  theme(legend.position = "right") + 
  scale_color_brewer(palette="Dark2", labels=c("Baseline", "NaiveBayes", "glmnet", "kNN", "SVM", "XGBoost")) + 
  #scale_fill_brewer(palette="Dark2", labels=c("Baseline", "NaiveBayes", "glmnet", "kNN", "SVM", "XGBoost")) +
  theme_bw()

#line size
prc_plot$layers[[2]]$aes_params$size=1

#supress confidence bands
prc_plot <- remove_geom(p=prc_plot, geom="smooth")

#call plot
prc_plot

```


## Conclusions
